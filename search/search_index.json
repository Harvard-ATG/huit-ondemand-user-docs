{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HUIT Open OnDemand","text":"<p>HUIT Open OnDemand is a new platform to provide access to high performance compute (HPC) resources for courses. It runs in AWS using ParallelCluster, unlike other HPC resources used for research that are housed in physical data centers. If you have access to the platform, you can log in via ood.huit.harvard.edu/</p> <p>HUIT Open OnDemand is only available for use in courses. If you want to use it in your course, you need to submit a request at atg.fas.harvard.edu/academic-computing. We need a new request for each semester that you intend to use the platform for, so that we can ensure that the platform is still needed, and that we're providing the software and compute resources required for your course.</p>"},{"location":"#interactive-software","title":"Interactive Software","text":"<p>In OnDemand, users have access to a file management app to access files in their OnDemand home directory through a browser. They also have access to a terminal app that provides browser access to a terminal in the cluster. In addition, we've set up additional interactive apps for different use cases, outlined on our interactive apps page</p>"},{"location":"#shared-command-line-software","title":"Shared Command Line Software","text":"<p>This platform uses Spack to manage software packages that run on compute nodes. For more information on the shared spack packages, see the Shared Spack Software documentation page.</p> <p>If you prefer to install software to your home directory, or if you want to test out new software configurations, you can install spack directly to your home directory. For more information on that, visit the Local Spack Installation documentation page.</p>"},{"location":"file-management/","title":"File Management","text":"<p>In Open OnDemand, you can access files in your home directory through a browser, rather than a terminal or through an application within Open OnDemand. This allows you to quickly upload or download files, and to make simple edits to text files in your home directory.</p> <p>To get to your files through the browser, use the Files dropdown menu in the bar at the top and click on \"Home Directory\".</p> <p></p> <p>In the home directory tool, you can upload, download, or manipulate your files.</p> <p></p>"},{"location":"interactive-apps/","title":"Interactive Apps","text":"<p>There are a number of interactive apps available in HUIT Open OnDemand. Which one you use will be determined by which class you are in. These apps have some common configuration elements that you'll see when launching any application.</p> <p></p> <p>In this screenshot, the configuration for the launch of the Code Server app, you'll see that you can specify a duration for which you want to have the app available, as well as the number of CPUs you want to allocate to the job.</p> <p>When selecting the duration for your session, choose a length of time a bit longer than the amount of time that you intend to work. The time cannot be extended, but you can release the resources that your session uses by exiting the interactive app from within its interface (the way you do this differs from app to app) or canceling the slurm job running it.</p> <p>When selecting the number of CPUs allocated to your job, allocate what works well for the application you're using. The benefits of allocating more resources to your session are pretty self-explanatory, but if you allocate a smaller number of CPUs, you have a better chance for your job to run on an existing compute node with available capacity, rather than having to wait several minutes while a new node comes online.</p> <p>In addition to these general settings, we also have application-specific documentation for the following apps:</p> <ul> <li>JupyterLab (Spack)</li> <li>Remote Desktop (ROS / Gazebo / Matlab)</li> <li>Code Server</li> </ul>"},{"location":"jupyterlab-spack/","title":"JupyterLab (spack)","text":"<p>The JupyterLab app that runs via spack has some options in addition to the basic CPU and time allocations. These allow you to launch a session with an alternative configuration that you define. </p> <p>If you just want to use the default packages included in the app, including <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, and <code>scipy</code>, then you don't need to worry about these settings. If you want to define your own environment with spack, or you have been instructed that an alternative environment has been prepared for your class, read on.</p>"},{"location":"jupyterlab-spack/#alternative-spack-installation","title":"Alternative Spack Installation","text":"<p>The package manager that we use in HUIT Open OnDemand, Spack, has a self-contained installation in the file system. That means that within your own home directory, or within a course shared folder, you can set up a different Spack installation with packages and environments that you define, and which can draw on the software already installed globally. The JupyterLab (spack) setup allows you to specify an alternative Spack path to use, so that the same app configuration can be used with these other installations.</p> <p>When setting this option, you must already have an alternative Spack installation set up, and the installation must include a named environment that includes the <code>py-jupyterlab</code> package. You can refer to this installation by its absolute path, beginning with <code>/shared</code></p>"},{"location":"jupyterlab-spack/#alternative-spack-environment","title":"Alternative Spack Environment","text":"<p>The JupyterLab (spack) application also allows you to specify an alternate Spack environment. Spack environments define a consistent set of packages in a more durable way than a series of <code>spack load</code> commands. You may be instructed to change this environment if one has been set up for your course as part of the global Spack installation. If you've set up your own Spack installation, you must also define an environment within that installation that includes the <code>py-jupyterlab</code> package, as well as any other packages containing Python modules that you want to have available. For more on managing packages with Spack, see the package management page</p>"},{"location":"package-management/","title":"Using Spack for Package Management","text":"<p>Spack is software that is pre-configured in HUIT Open OnDemand to enable the easy installation and management of high performance compute applications. There is a wide variety of software available, and for packages with existing spack configurations, the installation is usually just a one line command from the terminal.</p> <p>Managing a Spack installation will require heavy use of the terminal; there is no graphical interface available. If you need help with your course's environment, please reach out to atg@fas.harvard.edu.</p> <p>However, since installations can take a while, and since you likely want a consistent environment, there are some best practices for managing a spack installation. You can manage any spack installation that you have access to, whether it's a Spack installation in a course shared folder for a course that you're on the teaching staff for, or a personal home directory spack installation.</p>"},{"location":"package-management/#spack-environments","title":"Spack Environments","text":"<p>While you can just install packages directly to the Spack installations, Spack environments provide more stability, especially when being loaded automatically. For example, if you install one version of a package, load it with a <code>spack load</code> command in your <code>.bashrc</code> file, and later install another version of the same package, then the <code>spack load</code> command in your <code>.bashrc</code> can suddenly stop working. Environments have an internal lock file that articulates the exact packages that are loaded, so they're safe to load from a <code>.bashrc</code> file or in an automated script. When Spack is required for an interactive application, we use Spack environments for this reason.</p> <p>The Spack documentation provides a tutorial on environments, which is useful for a more comprehensive understanding of how these environments function. In this documentation, we'll take a more focused approach, dealing with common setup and installation tasks for preparing an environment for a course.</p> <p>You can only set up environments in environments that you can modify. In HUIT Open OnDemand, that means that you can only set up environments in your own personal Spack installation in your home folder, or in a shared course folder. In both cases, you should have the global spack installation set as an upstream, whether by the course shared folder's pre-made spack installation script, or by following our directions on setting up your own Spack installation.</p> <p>For the rest of this document, we'll assume that you've already sourced the <code>setup-env.sh</code> script from the installation where you want the environment to be.</p>"},{"location":"package-management/#creating-activating","title":"Creating &amp; Activating","text":"<p>For an environment to be used, it must first be created. This is pretty straightforward, with the command</p> <pre><code>spack env create environmentName\n</code></pre> <p>Replace \"environmentName\" with whatever you want the environment to be called. Then the environment can be activated with</p> <pre><code>spack env activate environmentName\n</code></pre>"},{"location":"package-management/#adding-packages","title":"Adding Packages","text":"<p>To get started, you can add one or more packages. You do this with a command like</p> <pre><code>spack add packageSpec\n</code></pre> <p>You can install a package simply by name, but if you have specific version or compiler requirements, you can specify those with Spack's spec syntax.</p> <p>Adding a package won't install it, but it will associate that package with the environment itself.</p>"},{"location":"package-management/#installing-packages","title":"Installing Packages","text":"<p>The syntax to start an installation is simple, but where to run the installation is less simple. It's best to run these installations in a Slurm job, since some installations will take a long time to run. Therefore, we recommend adapting this bash script for your Spack installation, and using it via <code>sbatch</code>. This is the script that we use to manage the global installation. You can adapt it for your use by copying the script to a file in your own home directory (we call it <code>spackEnvironment.sh</code>, so you might as well use the same name). Once you have your own copy of the script in HUIT OOD, you can edit the line marked with <code># CHANGEME</code> to point the script to your own Spack installation, rather than the global one.</p> <pre><code>#!/bin/bash\nif [ -x \"${1}\" ]\n  then\n    echo \"Environment name is required, e.g. `spackEnvironment.sh environmentName`\"\n    exit 1\nfi\n\n# Change the line below to activate your own Spack environment\n. /shared/spack/share/spack/setup-env.sh # CHANGEME\nENVIRONMENT=$1\necho \"Activating environment $ENVIRONMENT...\"\nspack env activate $ENVIRONMENT\necho \"Installing packages for $ENVIRONMENT...\"\nspack install\necho \"Done!\"\n</code></pre> <p>From wherever you place that script, assuming you name it <code>spackEnvironment.sh</code>, you can run it as a Slurm job with</p> <pre><code>sbatch -c 4 spackEnvironment.sh environmentName\n</code></pre> <p>Once you've submitted the job, you can check on the job's progress with slurm commands. For instance, you can see the state of your running jobs with the command <code>squeue -u $(whoami)</code>. A \"state\" of \"CF\" indicates your job is allocated to a job that is currently starting (or configuring), and a state of \"R\" indicates that your job is running. This command will also show you the job ID of your job, which you can use to view the output of your job. The output file will appear in the directory you started the job from as <code>slurm-$JOBID.out</code>, where <code>$JOBID</code> is your job ID.</p> <p>If you prefer to do this interactively, you can start an interactive Slurm session with an <code>salloc</code> command, then using <code>srun spack install</code>. However, this could require babysitting your terminal to see if there are any errors with the installation. By running the installation as a batch job, output from the install process is recorded to an output log file, so you can review what the script did at your leisure.</p>"},{"location":"package-management/#saving-an-environment-definition","title":"Saving an Environment Definition","text":"<p>If you have spent some time creating a Spack environment during the course of a term, you may want to re-use the same environment in future terms. Or, if you leave the university, you may want to save an environment definition to take with you to another institution to use on their infrastructure. Spack supports this with <code>spack.yaml</code> files.</p> <p>When you create an environment, a file is created at <code>$SPACK_ROOT/var/spack/environments/$ENVIRONMENT_NAME/spack.yaml</code>, where <code>$SPACK_ROOT</code> is the location of your spack installation, and <code>$ENVIRONMENT_NAME</code> is the name of the environment that you've set up. You can copy this file to another location where you can download it. Say you save it as MyEnvironment.yaml. Then, in the future, you can re-create the same environment with </p> <pre><code>spack env create environmentName MyEnvironment.yaml\n</code></pre>"},{"location":"remote-desktop-ros/","title":"Remote Desktop (ROS / Gazebo / Matlab)","text":"<p>This remote desktop application is configured with Matlab, as well as the robotics software ROS and Gazebo. It also comes with a code editor, VS Codium, and a standard Firefox web browser.</p> <p>The application launches with the standard settings for CPUs and the number of hours in the reservation. When the application is available, there are some additional options for your connection to the application. You can adjust the image quality and the compression level. These settings are visible in the screenshot below.</p> <p></p> <p>Compression adjusts the amount of image compression that your remote desktop connection is put through. Compression requires resources on the CPU hosting the remote desktop, but reduces the amount of bandwidth that the connection uses. If you're experiencing lagginess in your connection to the remote desktop, you can try increasing the compression, but you may need to allocate more CPUs than your work would otherwise require.</p> <p>Image quality is related to compression, but changes how the desktop appears. Low quality values will result in more compression artifacts in the image from the remote desktop, but can further reduce bandwidth. Keep this as high as you can if your connection is stable, or as low as you can tolerate if your internet connection is poor.</p>"},{"location":"remote-desktop-ros/#using-the-remote-desktop","title":"Using the remote desktop","text":"<p>You should be able to launch all of the applications that you need from their desktop icons. There is also a terminal that runs in the remote desktop if needed. Note that the remote desktop is running inside of a container, so it does not have access to applications available in the cluster itself, like Slurm commands or Spack packages.</p>"},{"location":"remote-desktop-ros/#black-screen-on-idle-sessions","title":"Black screen on idle sessions","text":"<p>There is a known issue with remote desktop sessions where if left idle for too long, they can fail to reconnect. If this happens, it's best to start a new session, then cancel the old one. If you've found a better solution for avoiding or re-connecting to sessions, please let us know at atg@fas.harvard.edu.</p> <p>To cancel the old session, start a terminal session and use the <code>scancel</code> command to cancel the job. In the example in the screenshot above, the job ID is 25 (visible in parentheses next to the app name), so in the terminal you would use <code>scancel 25</code> to cancel that session and free up those resources. Canceling unused sessions keeps the cluster's resources available for new sessions, for both you and your classmates.</p> <p>Logging out of a remote desktop session will also end the session, so it's the nice thing to do when you stop working.</p>"},{"location":"shared-folder/","title":"Shared Folder","text":"<p>In HUIT OnDemand, course staff have access to a shared folder for use in their course. Users in a course will have a link created to the shared folder for any course that they are enrolled in upon login to HUIT Open OnDemand. The shared folder is named for the Canvas course ID for the course they are enrolled in, usually a six digit number. This link is to the shared folder's absolute path, which is</p> <pre><code>/shared/courseSharedFolders/${CANVAS_COURSE_ID}outer/${CANVAS_COURSE_ID}\n</code></pre> <p>This uses <code>${CANVAS_COURSE_ID}</code> as a placeholder for the actual course ID.</p> <p>The shared folder should contain three utility scripts:</p> <ul> <li><code>fix-permissions.sh</code></li> <li><code>link_folder.sh</code></li> <li><code>setup_spack.sh</code></li> </ul> <p>The <code>fix-permissions.sh</code> script sets appropriate permissions on the contents of the folder, which is useful if students or teaching staff run into permisssions errors in the course.</p> <p>The <code>link_folder.sh</code> script will re-create the link to the shared folder for a user in the case where it is removed. If someone unintentionally removes the link to the shared folder, they can run this script from the absolute path to the shared folder to recreate it:</p> <pre><code>/shared/courseSharedFolders/${CANVAS_COURSE_ID}outer/${CANVAS_COURSE_ID}/link_folder.sh\n</code></pre> <p>The <code>setup_spack.sh</code> folder is there for course staff to set up a Spack installation for their course, and then link it to the global installation as an upstream. For more details about this kind of setup, see the course level Spack documentation.</p>"},{"location":"shared-folder/#support","title":"Support","text":"<p>If you have issues with your shared folder, please reach out to ithelp@harvard.edu. Please be sure to include the name of this platform, \"HUIT Open OnDemand\", to help your request get routed appropriately.</p>"},{"location":"spack-course-shared/","title":"Spack in shared course folder","text":"<p>HUIT Open OnDemand comes with Spack already installed, and we try to make sure that all of the software that instructors request is available from that global Spack installation. However, there may be cases where teaching staff want flexibility during the semester to install Spack packages on their own, while still sharing those installations with students. For this, it's possible to set up a Spack installation in the course shared folder provisioned by default for each course.</p> <p>Each shared course folder comes with a script, <code>setup_spack.sh</code>, which sets up a new Spack installation in the shared course folder and connects it to the global installation as an upstream source. This new Spack installation can be managed by course staff, without having to reinstall packages that are already installed globally.</p> <p>The way to activate the global installation is by sourcing a script from that global installation, like so:</p> <pre><code>. /shared/spack/share/spack/setup-env.sh\n</code></pre> <p>For a spack installation in a shared course folder, anyone with access can activate it by following the link in their home directory. For a class with course ID 000000, that would look like this:</p> <pre><code>. ~/000000/spack/share/spack/setup-env.sh\n</code></pre> <p>You should test the command to make sure it works first, but you can add a line like that one to your <code>~/.bashrc</code> file so that the Spack installation in your shared course folder is activated whenever you log in. Just be sure that it works, because problems with your <code>~/.bashrc</code> file can also cause problems with other interactive apps.</p> <p>Anyone on the teaching staff should have permission to modify the spack installation, including by installing new packages and creating new environments.</p> <p>For more on managing a Spack installation and Spack environments, see our Spack package management docs.</p>"},{"location":"spack-local/","title":"Using Spack From Your Home Directory","text":"<p>If you want to use different packages than the shared packages, or if you want to test out a package before requesting that it be installed in the shared context, you can also install a different <code>spack</code> to your home directory.</p>"},{"location":"spack-local/#installing","title":"Installing","text":"<p>Installing Spack just requires cloning its git repository:</p> <pre><code>git clone --depth=100 --branch=releases/v0.21 https://github.com/spack/spack.git ~/spack\n</code></pre> <p>You can change <code>~/spack</code> if you want to install to another location.</p> <p>Once you have Spack cloned, you can find the activation script in <code>&lt;install location&gt;/share/spack/setup-env.sh</code> That means if you installed spack via the command above, you can activate it with</p> <pre><code>. ~/spack/share/spack/setup-env.sh\n</code></pre>"},{"location":"spack-local/#setting-an-upstream","title":"Setting an upstream","text":"<p>You can build all of your own dependencies in your home folder, but it can be more expedient to use the pre-build software in the shared spack installation as a starting point. To enable this, you need to set the shared installation directory as an upstream. The easiest way to do this is to run this command with your local spack environment activated:</p> <pre><code>spack config add upstreams:spack-instance-1:install_tree:/shared/spack/opt/spack\n</code></pre>"},{"location":"spack-local/#adding-packages","title":"Adding packages","text":"<p>With a spack installation in your home directory, you can manage your own package installations. You can look for packages with <code>spack list &lt;packagename&gt;</code>, see your installed packages with <code>spack find</code>, and add new packages with <code>spack install &lt;packagename&gt;</code></p> <p>When using <code>spack</code> installed to your home directory, make sure that any scripts that reference local packages activate your spack environment, not the shared environment.</p> <p>For more on managing a Spack installation and Spack environments, see our Spack package management docs.</p>"},{"location":"spack-shared-software/","title":"Shared Software with Spack","text":"<p>Our configuration of OnDemand uses a centrally configured repository of command-line software managed by and available through Spack. We provide some pre-installed packages, but if you find that you need some software that is available through Spack, it can be installed to that shared location on request.</p>"},{"location":"spack-shared-software/#make-the-spack-command-available","title":"Make the <code>spack</code> command available","text":"<p>In order to avoid interfering with any other configurations that you may want to set up, Spack is not configured in your terminal by default. Rather, it must be activated with the following command (note the dot at the start, and be sure to include that with the rest):</p> <pre><code>. /shared/spack/share/spack/setup-env.sh\n</code></pre> <p>This will make the <code>spack</code> command available in your terminal session. If you want to use the shared <code>spack</code> command in every session, you can add this line to your <code>~/.bashrc</code> file.</p> <p>The Code Server app runs using a Spack package, so the shared <code>spack</code> command is available by default in the terminal inside the VS Code interface.</p>"},{"location":"spack-shared-software/#load-packages-with-spack","title":"Load packages with <code>spack</code>","text":"<p>With the <code>spack</code> command enabled, you can now load packages that have been installed to the shared location. You can list all of the packages available with <code>spack find</code>:</p> <pre><code>$ spack find\n-- linux-amzn2-skylake_avx512 / gcc@7.3.1 -----------------------\nberkeley-db@18.1.40                 curl@8.4.0      htslib@1.17         libdivsufsort@2.0.1  libtool@2.4.7     nghttp2@1.57.0       r@4.3.0               unzip@6.0\nboost@1.83.0                        curl@8.4.0      hwloc@2.9.1         libffi@3.4.4         libunistring@1.1  openjdk@11.0.20.1_1  readline@8.2          util-linux-uuid@2.38.1\nbowtie@1.3.1                        diffutils@3.9   icu4c@67.1          libgff@2.0.0         libxcrypt@4.4.35  openssl@3.1.3        salmon@1.10.2         util-macros@1.19.3\nbzip2@1.0.8                         expat@2.5.0     intel-pin@3.27      libiconv@1.17        libxml2@2.10.3    pcre2@10.42          sqlite@3.43.2         which@2.21\nca-certificates-mozilla@2023-05-30  gdbm@1.23       intel-tbb@2021.9.0  libidn2@2.3.4        likwid@5.2.2      perl@5.38.0          staden-io-lib@1.14.8  xz@5.4.1\ncereal@1.3.2                        gettext@0.22.3  jemalloc@5.3.0      libmd@1.0.4          lua@5.4.4         pigz@2.7             star@2.7.10b          zlib-ng@2.1.4\ncmake@3.27.7                        gmake@4.4.1     libbsd@0.11.7       libpciaccess@0.17    m4@1.4.19         pkgconf@1.9.5        tar@1.34              zstd@1.5.5\ncode-server@4.12.0                  gzip@1.12       libdeflate@1.18     libsigsegv@2.14      ncurses@6.4       python@3.11.6        texinfo@7.0.3\n==&gt; 63 installed packages\n</code></pre> <p>You can load packages with <code>spack load</code> followed by the name of the package. Once you have the package loaded, you'll be able to use the commands that it enables. As an example:</p> <pre><code>$ which likwid-perfctr\n/usr/bin/which: no likwid-perfctr in (/opt/amazon/openmpi/bin:...)\n$ spack load likwid\n$ which likwid-perfctr\n/shared/spack/opt/spack/linux-amzn2-skylake_avx512/gcc-7.3.1/likwid-5.2.2-aayxcqg6nj5zykdozo5z4yubzjevxhhm/bin/likwid-perfctr\n</code></pre>"},{"location":"spack-shared-software/#use-in-scripts","title":"Use in scripts","text":"<p>If you are preparing a batch job, be sure to include a line to make the <code>spack</code> command available, and to load the packages that your job needs.</p>"},{"location":"spack-shared-software/#downstream-installations","title":"Downstream installations","text":"<p>Spack is self-contained in a single folder, meaning you can install it yourself in your home directory, or in a course shared folder. This allows you to add your own software without having to request it from Academic Technology and wait for it to be set up. Both of these options are described in more detail in the course-level documentation and the personal spack installation documentation. In both cases, you can set the global installation as an \"upstream\" installation, meaning that you don't have to waste time and effort installing packages that are already set up globally, while still having the power to install your own software.</p>"},{"location":"terminal/","title":"Terminal App","text":"<p>Open OnDemand provides access to a browser-based terminal. This terminal runs on a login node in the compute cluster, rather than a compute node, so the terminal environment should not be used directly for computation.</p> <p>Instead, the terminal should be used to queue batch or interactive jobs with slurm commands, or to perform simple, non-intensive tasks in your home directory.</p> <p>To access the terminal app, go to the \"Clusters\" menu item and click on \"academic Shell Access\". This should open a new tab with a terminal interface into an Amazon Linux 2 environment.</p> <p></p> <p>When you connect to the terminal, you should see something similar to the screenshot below. If you are prompted for a password, that indicates that a problem has occurred, so please reach out to support via atg@fas.harvard.edu</p> <p></p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#undetermined-state-interactive-apps","title":"\"Undetermined\" State Interactive Apps","text":"<p>When launching an interactive app, you may see an information card like this one:</p> <p></p> <p>If this happens, it is usually because of a problem in your <code>.bashrc</code> file. If you use the dashboard terminal app to connect to a login node, you should see the full error causing the problem. The error message should point to a solution to apply to your <code>.bashrc</code> file, but if you have trouble finding a solution, reach out to ithelp@harvard.edu for further assistance.</p> <p>With a working <code>.bashrc</code> file, interactive apps should start functioning normally. However, sessions that you started will still be running. You can cancel these sessions by first finding their job ID with the <code>squeue</code> command in the dashboard terminal to see your running jobs with IDs. Then, run <code>scancel {jobid}</code> (don't include the curly braces) to cancel the running job.</p> <p>The \"Undetermined\" state cards will also persist in your interactive session history. If you wish to clean them up and remove them, take note of the session ID in the card. You can remove the corresponding file in your home directory with a <code>rm ~/ondemand/data/sys/dashboard/batch_connect/db/{session_id}</code> command from the dashboard terminal or any interactive app with terminal access.</p>"},{"location":"vscode-app/","title":"VS Code App (Code Server)","text":"<p>We have added a Visual Studio Code app to the Open OnDemand interface, to provide a full-featured IDE with access to the cluster through your browser. You can set this app to run for a pre-determined amount of time, and can allocate up to 4 CPUs from the cluster to your interactive session (although we do ask that you do your best not to allocate more resources than you need).</p> <p>To start, go to the \"Interactive Apps\" menu and select \"Code Server\"</p> <p></p> <p>You'll then be presented with some options for how many CPUs you need, and how long you expect to work for. Pick your options, then click \"Launch\"</p> <p></p> <p>You should see a screen like the screenshot below, where your Code Server session will appear queued. Depending on the resources available in the platform, your session may take several minutes to start as new compute instances are prepared for use.</p> <p></p> <p>Once your compute resources have been allocated and your environment is ready, you'll see that your session is \"Running\", and you'll have a button to launch. Click \"Connect to VS Code\" to access the VS Code interface.</p> <p></p> <p>Once you click the button to connect, you should be directed to a VS Code interface like this one. You shouldn't be prompted for a password, and if you are, that indicates an issue with how the VS Code app is working, so please reach out to support via atg@fas.harvard.edu</p> <p></p>"},{"location":"vscode-app/#slurm-commands-from-vs-code-interface","title":"Slurm commands from VS Code interface","text":"<p>When in the terminal in VS Code, you have access to the same slurm commands that are available in the terminal that runs on the login node, accessible from the OnDemand dashboard. However, the commands may behave differently, since the VS Code interface is running as part of an existing slurm job. For instance, you cannot allocate additional CPUs via regular <code>srun</code> commands, beyond what the VS Code interface is already running. However, <code>sbatch</code> commands can dispatch jobs with additional resources from this context. If you find other problematic slurm behavior from the VS Code interface, especially if you have found a workaround, let us know by opening an issue in the repository for this documentation: github.com/Harvard-ATG/huit-ondemand-user-docs/issues</p>"}]}